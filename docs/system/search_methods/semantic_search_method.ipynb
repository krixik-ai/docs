{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.semantic_search` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krixik's `.semantic_search` method enables semantic search on documents processed through certain pipelines. Much has been written about semantic search, but in a nutshell, instead of searching a document for specific keywords, it searches for text similar in _meaning_ to the string that's been queried for. Contrast this to [keyword search](../system/search_methods/keyword_search_method.md).\n",
    "\n",
    "Given that the `.semantic_search` method both [embeds](../modules/ai_model_modules/text-embedder_module.md) the query and performs the search, it can only be used with pipelines containing both a [`text-embedder`](../modules/ai_model_modules/text-embedder_module.md) module and a [`vector-db`](../modules/database_modules/vector-db_module.md) module in immediate succession.\n",
    "\n",
    "This overview of the `.semantic_search` method is divided into the following sections:\n",
    "\n",
    "- [.semantic_search Method Arguments](#.semantic_search-method-arguments)\n",
    "- [Example Pipeline Setup and File Processing](#example-pipeline-setup-and-file-processing)\n",
    "- [Example Semantic Searches](#example-semantic-searches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.semantic_search` Method Arguments\n",
    "\n",
    "The `.semantic_search` method takes one required argument and at least one of several optional arguments. The required argument is:\n",
    "\n",
    "- `query` (str) - A string whose meaning will be searched for across the target document. The closest matches (i.e. snippets of text that most match the query in meaning) will be returned.\n",
    "\n",
    "The optional arguments are the same arguments that the [`.list`](../system/file_system/list_method.md) method takes—both metadata and timestamp bookends—so please take a moment to [review them here](../system/file_system/list_method.md#.list-method-arguments). As with the [`.list`](../system/file_system/list_method.md) method, you can semantically search across several files at the same time because all metadata arguments are submitted to the `.semantic_search` method in list format. All optional argument elements are the same as for the [`.list`](../system/file_system/list_method.md) method, including the wildcard operator and the global root.\n",
    "\n",
    "If none of these optional arguments is present, the `.semantic_search` method will not work because there will be nothing to search through.\n",
    "\n",
    "Like the [`.list`](../system/file_system/list_method.md) method, the `.semantic_search` method also accepts the optional `max_files` and `sort_order` arguments, though their function changes a bit:\n",
    "\n",
    "- `max_files` - Specifies up to how many files should be searched through. Default is none.\n",
    "\n",
    "- `sort_order` - Here takes three possible values: 'ascending', descending', and now 'global'. The first two sort results by the file they're in (the files are sorted by the creation timestamp of the file), and 'global' combines all files and returns the very best results across all files. Default is 'descending'.\n",
    "\n",
    "Finally, the `.semantic_search` method accepts one optional method that is unique to it:\n",
    "\n",
    "- `k` (int) - Specifies up to how many results will be returned per queried file. Default is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Pipeline Setup and File Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this document's examples we will use a pipeline consisting of three modules: a [`parser module`](../modules/ai_model_modules/parser_module.md), a [`text-embedder module`](../modules/modules/ai_model_modules/text-embedder_module.md), and a [`vector-db module`](../modules/database_modules/vector-db_module.md). This is the basic semantic search [pipeline](../examples/search_pipeline_examples/multi_basic_semantic_search.md). We use the [`create_pipeline`](../system/pipeline_creation/create_pipeline.md) method to instantiate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the basic semantic search pipeline\n",
    "\n",
    "pipeline_1 = krixik.create_pipeline(name=\"semantic_search_method_1_parser_text-embedder_vector-db\",\n",
    "                                    module_chain=[\"parser\", \"text-embedder\", \"vector-db\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline ready, we'll [`.process`](../system/parameters_processing_files_through_pipelines/process_method.md) a few text files through it so we have something to search through. Let's use the same files we used in the [`.list` method documentation](../system/file_system/list_method.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add four files to the pipeline we just created.\n",
    "\n",
    "output_1 = pipeline_1.process(local_file_path=\"../../data/input/Frankenstein.txt\", # the initial local filepath where the input JSON file is stored\n",
    "                              expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "                              wait_for_process=True,  # do not wait for process to complete before returning IDE control to user\n",
    "                              verbose=False,  # do not display process update printouts upon running code\n",
    "                              symbolic_directory_path=\"/novels/gothic\",\n",
    "                              file_name=\"Frankenstein.txt\")\n",
    "\n",
    "output_2 = pipeline_1.process(local_file_path=\"../../data/input/Pride and Prejudice.txt\", # the initial local filepath where the input JSON file is stored\n",
    "                              expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "                              wait_for_process=True,  # do not wait for process to complete before returning IDE control to user\n",
    "                              verbose=False,  # do not display process update printouts upon running code\n",
    "                              symbolic_directory_path=\"/novels/romance\",\n",
    "                              file_name=\"Pride and Prejudice.txt\")\n",
    "\n",
    "output_3 = pipeline_1.process(local_file_path=\"../../data/input/Moby Dick.txt\", # the initial local filepath where the input JSON file is stored\n",
    "                              expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "                              wait_for_process=True,  # do not wait for process to complete before returning IDE control to user\n",
    "                              verbose=False,  # do not display process update printouts upon running code\n",
    "                              symbolic_directory_path=\"/novels/adventure\",\n",
    "                              file_name=\"Moby Dick.txt\")\n",
    "\n",
    "output_4 = pipeline_1.process(local_file_path=\"../../data/input/Little Women.txt\", # the initial local filepath where the input JSON file is stored\n",
    "                              expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "                              wait_for_process=True,  # do not wait for process to complete before returning IDE control to user\n",
    "                              verbose=False,  # do not display process update printouts upon running code\n",
    "                              symbolic_directory_path=\"/novels/bildungsroman\",\n",
    "                              file_name=\"Little Women.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output for one of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nicely print the output of one of the above processes\n",
    "\n",
    "print(json.dumps(output_2, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of `process_output` is `null` because the return object is a database, so it cannot be printed here. You can review this database in the local location provided in the `process_output_files`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Semantic Searches\n",
    "\n",
    "With files now processed through the pipeline we can run the `.semantic_search` method on it.\n",
    "\n",
    "Let's try an example in which we query one of the files file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"10503c1c-3959-4897-9315-a69438ecce2b\",\n",
      "  \"message\": \"Successfully queried 1 user file.\",\n",
      "  \"warnings\": [],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"f69aac3d-e674-45d5-ab33-f16196ce82b2\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"krixik_generated_file_name_awiouirlff.txt\",\n",
      "        \"symbolic_directory_path\": \"/etc\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_vectors\": 2,\n",
      "        \"created_at\": \"2024-04-26 21:10:50\",\n",
      "        \"last_updated\": \"2024-04-26 21:10:50\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.224\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "          \"line_numbers\": [\n",
      "            2,\n",
      "            3,\n",
      "            4,\n",
      "            5\n",
      "          ],\n",
      "          \"distance\": 0.417\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform semantic_search over one file\n",
    "\n",
    "semantic_output_1 = pipeline_1.semantic_search(query=\"It was cold night.\",\n",
    "                                               file_names=[\"Little Women.txt\"])\n",
    "\n",
    "# nicely print the output of this search\n",
    "\n",
    "print(json.dumps(semantic_output_1, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to returning the snippets that are closest in meaning to our query, we also see the calculated vector distance (in a way, the distance in meaning) between each result and the query. The shorter this distance is, the closer in meaning the result to the query. The `.semantic_search` method returns the snippets with the shortest vector distance to query, ranked in ascending order within each file.\n",
    "\n",
    "When `sort_order` is set to 'global', results from all files are combined and the method returns the snippets with the shortest distance to query, ranked in ascending order, regardless of what file each result may be in. Let's give this a shot by searching through multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"10503c1c-3959-4897-9315-a69438ecce2b\",\n",
      "  \"message\": \"Successfully queried 1 user file.\",\n",
      "  \"warnings\": [],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"f69aac3d-e674-45d5-ab33-f16196ce82b2\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"krixik_generated_file_name_awiouirlff.txt\",\n",
      "        \"symbolic_directory_path\": \"/etc\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_vectors\": 2,\n",
      "        \"created_at\": \"2024-04-26 21:10:50\",\n",
      "        \"last_updated\": \"2024-04-26 21:10:50\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.224\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "          \"line_numbers\": [\n",
      "            2,\n",
      "            3,\n",
      "            4,\n",
      "            5\n",
      "          ],\n",
      "          \"distance\": 0.417\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform semantic_search over multiple files\n",
    "\n",
    "semantic_output_2 = pipeline_1.semantic_search(query=\"It was cold night.\",\n",
    "                                               symbolic_directory_paths=[\"/novels*\"],\n",
    "                                               sort_order='global'\n",
    "                                               k=4)\n",
    "\n",
    "# nicely print the output of this search\n",
    "\n",
    "print(json.dumps(semantic_output_2, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, results from all the files have been combined, and the result ranked at the top has the shortest query-result distance of the entire file set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "\n",
    "reset_pipeline(pipeline_1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
