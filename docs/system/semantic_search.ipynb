{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `semantic_search` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "krixik's `semantic_search` method is a convenience function for both embedding and querying - and so can only be used with pipelines containing both [`text-embedder`](modules/text-embedder.md) and [`vector-db`](modules/vector-db.md) modules in succession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A table of contents for the remainder of this document is shown below.\n",
    "\n",
    "- [basic pipeline setup](#basic-pipeline-setup)\n",
    "- [basic usage, required input, and output breakdown](#basic-usage,-required-input,-and-output-breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# import utilities\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n",
    "\n",
    "# load secrets from a .env file using python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "MY_API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "MY_API_URL = os.getenv(\"MY_API_URL\")\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "\n",
    "krixik.init(api_key=MY_API_KEY, api_url=MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pipeline setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this document we will use a pipeline consisting of three modules: a [`parser`](modules/parser.md), [`text-embedder`](modules/text-embedder.md), and [`vector-db`](modules/vector-db.md) index.  We use [`create_pipeline`](system/create_save_load.md) to instantiate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with multiple modules\n",
    "pipeline = krixik.create_pipeline(\n",
    "    name=\"vector-search-system-intro\",\n",
    "    module_chain=[\"parser\", \"text-embedder\", \"vector-db\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage, required input, and output breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the usage of `semantic_search` we process a short file illustrated in the introduction to the [`parser` method](modules/parser.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/1984_very_short.txt\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=test_file,\n",
    "    local_save_directory=\"../../data/output\",  # save output repo data output subdir\n",
    "    expire_time=60 * 10,  # set all process data to expire in 10 minutes\n",
    "    wait_for_process=True,  # wait for process to complete before regaining ide\n",
    "    verbose=False,\n",
    ")  # set verbosity to False\n",
    "\n",
    "#\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the returned output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"file_id\": \"3d435c55-05ae-41b6-aee3-76da8c7b0841\",\\n  \"request_id\": \"5e723bee-4939-21f1-52ef-ca0596dd3f1f\",\\n  \"file_name\": \"krixik_generated_file_name_vplttsahnp.txt\",\\n  \"symbolic_directory_path\": \"/etc\",\\n  \"file_tags\": null,\\n  \"file_description\": null\\n}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first process a file with our new pipeline.  The [`vector-db`](modules/vector-db.md) module takes in a text file, and returns `faiss` vector database consisting of all non-trivial `(snippet, line_numbers)` tuples from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"vector-search-pipeline-1\",\n",
      "  \"request_id\": \"1a09068c-872a-4389-a399-7281e2d1764e\",\n",
      "  \"file_id\": \"f69aac3d-e674-45d5-ab33-f16196ce82b2\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id f69aac3d-e674-45d5-ab33-f16196ce82b2.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"./f69aac3d-e674-45d5-ab33-f16196ce82b2.faiss\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/1984_very_short.txt\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=test_file,\n",
    "    local_save_directory=\"../../data/output\",  # save output repo data output subdir\n",
    "    expire_time=60 * 10,  # set all process data to expire in 10 minutes\n",
    "    wait_for_process=True,  # wait for process to complete before regaining ide\n",
    "    verbose=False,\n",
    ")  # set verbosity to False\n",
    "\n",
    "# nicely print the output of this process\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not define a `file_name` or `symbolic_directory_path` ourselves, so defaults will be given as described in the `.process` walkthrough [LINK HERE]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `process_output` key value is `null` since the return object is a database.  We can see this database in the local location provided in the `process_output_files` value.\n",
    "\n",
    "With `.process` complete we can run `semantic_search` on our input file. \n",
    "\n",
    "The `semantic_search` method takes in the exact same arguments as [`list`](system/list.md) - that is `file_ids`, `file_names`, etc., - plus one additional argument: `query`.  The `query` is a string of words to be queried individually.\n",
    "\n",
    "Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"10503c1c-3959-4897-9315-a69438ecce2b\",\n",
      "  \"message\": \"Successfully queried 1 user file.\",\n",
      "  \"warnings\": [],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"f69aac3d-e674-45d5-ab33-f16196ce82b2\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"krixik_generated_file_name_awiouirlff.txt\",\n",
      "        \"symbolic_directory_path\": \"/etc\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_vectors\": 2,\n",
      "        \"created_at\": \"2024-04-26 21:10:50\",\n",
      "        \"last_updated\": \"2024-04-26 21:10:50\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.224\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "          \"line_numbers\": [\n",
      "            2,\n",
      "            3,\n",
      "            4,\n",
      "            5\n",
      "          ],\n",
      "          \"distance\": 0.417\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform semantic_search over the input file\n",
    "semantic_output = pipeline.semantic_search(\n",
    "    query=\"it was cold night\", file_ids=[process_output[\"file_id\"]]\n",
    ")\n",
    "\n",
    "# nicely print the output of this process\n",
    "print(json.dumps(semantic_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one returned search result in `items`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
