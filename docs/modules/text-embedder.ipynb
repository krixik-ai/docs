{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `text-embedder` module\n",
    "\n",
    "This document reviews the `text-embedder` module - which takes as input a json of text snippets, transforms each into a vector, and returns an array of those vectors in a .npy flie.\n",
    "\n",
    "This document includes an overview of custom pipeline setup, current model set, parameters, and `.process` usage for this module.\n",
    "\n",
    "To follow along with this demonstration be sure to initialize your krixik session with your api key and url as shown below. \n",
    "\n",
    "We illustrate loading these required secrets in via [python-dotenv](https://pypi.org/project/python-dotenv/), storing those secrets in a `.env` file.  This is always good practice for storing / loading secrets (e.g., doing so will reduce the chance you inadvertantly push secrets to a repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../../')\n",
    "import importlib\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "# load secrets from a .env file using python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"../.env\")\n",
    "MY_API_KEY = os.getenv('MY_API_KEY')\n",
    "MY_API_URL = os.getenv('MY_API_URL')\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "krixik.init(api_key = MY_API_KEY, \n",
    "            api_url = MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small function prints dictionaries very nicely in notebooks / markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dictionaries / json nicely in notebooks / markdown\n",
    "import json\n",
    "def json_print(data):\n",
    "    print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A table of contents for the remainder of this document is shown below.\n",
    "\n",
    "- [pipeline setup](#pipeline-setup)\n",
    "- [required input format](#required-input-format)\n",
    "- [using the default model](#using-the-default-model)\n",
    "- [examining process output locally](#examining-process-output-locally)\n",
    "- [processing with a non-default model](#processing-with-a-non-default-model)\n",
    "- [using a non-default model](#using-a-non-default-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline setup\n",
    "\n",
    "Below we setup a simple one module pipeline using the `text-embedder` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with a single module\n",
    "pipeline = krixik.create_pipeline(name=\"my-text-embedder-pipeline\",\n",
    "                                  module_chain=[\"text-embedder\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `text-embedder` module comes with a five very popular models from huggingface.  Each model functions in the same general manner - transforming text into dense vectors.\n",
    "\n",
    "- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) (default)\n",
    "- [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n",
    "- [all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)\n",
    "- [multi-qa-MiniLM-L6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) \n",
    "- [msmarco-distilbert-dot-v5](https://huggingface.co/sentence-transformers/msmarco-distilbert-dot-v5)\n",
    "\n",
    "Quantized versions of each are also available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These available modeling options and parameters are stored in our custom pipeline's configuration (described further in LINK HERE).  We can examine this configuration as shown below.\n",
    "\n",
    "Notice each model has a single parameter - `quantize` - that can be set to a boolean value `True/False`.  By default the `quantize` is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipeline\": {\n",
      "    \"name\": \"my-text-embedder-pipeline\",\n",
      "    \"modules\": [\n",
      "      {\n",
      "        \"name\": \"text-embedder\",\n",
      "        \"models\": [\n",
      "          {\n",
      "            \"name\": \"all-MiniLM-L6-v2\",\n",
      "            \"params\": {\n",
      "              \"quantize\": {\n",
      "                \"type\": \"bool\",\n",
      "                \"default\": true\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"all-mpnet-base-v2\",\n",
      "            \"params\": {\n",
      "              \"quantize\": {\n",
      "                \"type\": \"bool\",\n",
      "                \"default\": true\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"all-MiniLM-L12-v2\",\n",
      "            \"params\": {\n",
      "              \"quantize\": {\n",
      "                \"type\": \"bool\",\n",
      "                \"default\": true\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"multi-qa-MiniLM-L6-cos-v1\",\n",
      "            \"params\": {\n",
      "              \"quantize\": {\n",
      "                \"type\": \"bool\",\n",
      "                \"default\": true\n",
      "              }\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"msmarco-distilbert-dot-v5\",\n",
      "            \"params\": {\n",
      "              \"quantize\": {\n",
      "                \"type\": \"bool\",\n",
      "                \"default\": true\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"defaults\": {\n",
      "          \"model\": \"all-MiniLM-L6-v2\",\n",
      "          \"params\": {\n",
      "            \"quantize\": true\n",
      "          }\n",
      "        },\n",
      "        \"input\": {\n",
      "          \"type\": \"json\",\n",
      "          \"permitted_extensions\": [\n",
      "            \".json\"\n",
      "          ]\n",
      "        },\n",
      "        \"output\": {\n",
      "          \"type\": \"npy\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the configuration of uor custom pipeline\n",
    "json_print(pipeline.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the models and their associated parameters available for use.\n",
    "\n",
    "You can save this configuration to disk as well by executing\n",
    "\n",
    "\n",
    "```python\n",
    "pipeline.save_pipeline(\"/valid/path/file.yml\")\n",
    "```\n",
    "\n",
    "You can instantiate a pipeline directly from its configuration using the [.load_pipeline method](LINK HERE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required input format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `text-embedder` module accepts as input `.json` files consisting of a *list of dictionaries*.  Each dictionary may have as many key-value pairs as desired, but *must* contain the key name *snippet*.  This is the key `text-embedder` will act on.\n",
    "\n",
    "Optionally, you may also include a key `line_numbers` containing a list of `integer` line numbers associated with the snippet.\n",
    "\n",
    "Let's look at an example of a small valid input - and then process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "    \"line_numbers\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "    \"line_numbers\": [\n",
      "      2,\n",
      "      3,\n",
      "      4,\n",
      "      5\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# examine contents of a valid input file\n",
    "test_file = \"../../data/input/1984_very_short.json\"\n",
    "with open(test_file) as f:\n",
    "    json_print(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process our small input example using the `default` model: .  Because [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) (quantized) is the default model we need not input the optional `modules` argument into `.process`.  Afterwords we will process the same file again, but select our model and quantization explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/1984_very_short.json\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(local_file_path = test_file,\n",
    "                                  local_save_directory=\"../../data/output\", # save output repo data output subdir\n",
    "                                  expire_time=60 * 10,      # set all process data to expire in 10 minutes\n",
    "                                  wait_for_process=True,    # wait for process to complete before regaining ide\n",
    "                                  verbose=False)            # set verbosity to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below.  \n",
    "\n",
    "Because the output of this particular module-model is a `.npy` file embedding vectors of the input, the process output is provided in this object is null.  However these files have been returned to the address noted in the `process_output_files` key.  The `file_id` of the processed file is used as a filename prefix for both output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"my-text-embedder-pipeline\",\n",
      "  \"request_id\": \"3517d9da-375e-4824-8212-fc9bebfb7c74\",\n",
      "  \"file_id\": \"525ee760-86ca-4b96-8bd9-46f905b85590\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id 525ee760-86ca-4b96-8bd9-46f905b85590.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../data/output/525ee760-86ca-4b96-8bd9-46f905b85590.npy\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "json_print(process_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining process output locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.npy` containing embedding vectors of our input data can be examined as follows.  For the sake of clarity we will simply print the shape of the returned array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "# examine vector output\n",
    "import numpy as np\n",
    "vectors = np.load(process_output['process_output_files'][0])\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each row of the returned array is an individual vector matching the index of the input.\n",
    "\n",
    " e.g., the first row is the vectorized form of our first input snippet shown above: \"It was a bright cold day in April, and the clocks were striking thirteen.\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing with a non-default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process with a non-default model include the `modules` input argument defining your choice of model and quantization.\n",
    "\n",
    "For example if we wish to process with [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) un-quantized this would new input argument would take the form\n",
    "\n",
    "```\n",
    "modules={\n",
    "        \"text-embedder\":\n",
    "            {\n",
    "                \"model\": \"all-mpnet-base-v2\",\n",
    "                \"params\":{\"quantize\": False}\n",
    "                }\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/1984_very_short.json\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(local_file_path = test_file,\n",
    "                                  local_save_directory=\"../../data/output\", # save output repo data output subdir\n",
    "                                  expire_time=60 * 10,      # set all process data to expire in 10 minutes\n",
    "                                  wait_for_process=True,    # wait for process to complete before regaining ide\n",
    "                                  verbose=False,            # set verbosity to False\n",
    "                                  modules={\"text-embedder\":\n",
    "                                            {\"model\": \"all-mpnet-base-v2\",\n",
    "                                             \"params\":{\"quantize\": False}}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine the output as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"my-text-embedder-pipeline\",\n",
      "  \"request_id\": \"008e70a1-9b55-4fe0-9b02-55dd4b02798e\",\n",
      "  \"file_id\": \"a7e7dd02-fd28-4a8e-8fae-eb495db0bd86\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id a7e7dd02-fd28-4a8e-8fae-eb495db0bd86.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../data/output/a7e7dd02-fd28-4a8e-8fae-eb495db0bd86.npy\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "json_print(process_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
