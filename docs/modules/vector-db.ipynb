{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `vector-db` module\n",
    "\n",
    "This document reviews the `vector-db` module - which takes as input a numpy array, indexes its vectors, and returns an indexed [faiss database](https://github.com/facebookresearch/faiss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A table of contents for the remainder of this document is shown below.\n",
    "\n",
    "\n",
    "- [pipeline setup](#pipeline-setup)\n",
    "- [required input format](#required-input-format)\n",
    "- [using the default model](#using-the-default-model)\n",
    "- [using the `semantic_search` method](#using-the-semantic_search-method)\n",
    "- [querying output databases locally](#querying-output-databases-locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# import utilities\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n",
    "\n",
    "# load secrets from a .env file using python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "MY_API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "MY_API_URL = os.getenv(\"MY_API_URL\")\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "\n",
    "krixik.init(api_key=MY_API_KEY, api_url=MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline setup\n",
    "\n",
    "Below we setup a simple one module pipeline using the `vector-db` module.\n",
    "\n",
    "We do this by passing the module name to the `module_chain` argument of [`create_pipeline`](../system/create_save_load.md) along with a name for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with a single module\n",
    "pipeline = krixik.create_pipeline(name=\"modules-vector-db-docs\", module_chain=[\"vector-db\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vector-db` module comes with a single model:\n",
    "\n",
    "- `faiss`: (default) indexes a numpy array of input vectors\n",
    "\n",
    "These available modeling options and parameters are stored in your custom [pipeline's configuration](../system/create_save_load.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required input format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vector-db` module accepts `.npy` consisting of a single numpy array.  Each row is a vector to be indexed for vector search.\n",
    "\n",
    "Let's look at an example of a small valid input - and then process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine contents of input file\n",
    "import numpy as np\n",
    "\n",
    "test_file = \"../../data/input/vectors.npy\"\n",
    "np.load(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process it using the default model - `faiss`.  Because `faiss` is the default model we need not input the optional `modules` argument into `.process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/vectors.npy\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=test_file,\n",
    "    local_save_directory=\"../../data/output\",  # save output repo data output subdir\n",
    "    expire_time=60 * 10,  # set all process data to expire in 5 minutes\n",
    "    wait_for_process=True,  # wait for process to complete before regaining ide\n",
    "    verbose=False,\n",
    ")  # set verbosity to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below.  Because the output of this particular module-model pair is a faiss database, the process output is provided in this object is null.  However the file itself has been returned to the address noted in the `process_output_files` key.  The `file_id` of the processed input is used as a filename prefix for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"my-vector-db-pipeline\",\n",
      "  \"request_id\": \"702f3ae8-4d2c-4723-9677-dd42133baba3\",\n",
      "  \"file_id\": \"a98fc86e-204a-428d-9144-d929624e2f5a\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id a98fc86e-204a-428d-9144-d929624e2f5a.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../data/output/a98fc86e-204a-428d-9144-d929624e2f5a.faiss\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying output databases locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform queries on the pulled vector database whose location is given in `process_output_files`.\n",
    "\n",
    "Below is a simple function for performing single keyword queries on this database locally.  Note: you will need to install the faiss library to execute this cell.  Install [faiss-cpu](https://pypi.org/project/faiss-cpu/) or [faiss-gpu](https://pypi.org/project/faiss-gpu/) depending on the specs of your local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you install faiss (faiss-cpu or faiss-gpu)\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def query_vector_db(query_vector: np.ndarray, k: int, db_file_path: str) -> Tuple[list, list]:\n",
    "    # read in vector db\n",
    "    faiss_index = faiss.read_index(db_file_path)\n",
    "\n",
    "    # perform query\n",
    "    similarities, indices = faiss_index.search(query_vector, k)\n",
    "    distances = 1 - similarities\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple query using the test function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input query vector: [0 1]\n",
      "closest vector from original: [0 1]\n",
      "distance from query to this vector: 0.0\n",
      "second closest vector from original: [1 1]\n",
      "distance from query to this vector: 0.2928932309150696\n"
     ]
    }
   ],
   "source": [
    "# perform test query using the above query function\n",
    "original_vectors = np.load(test_file)\n",
    "query_vector = np.array([[0, 1]])\n",
    "distances, indices = query_vector_db(query_vector, 2, process_output[\"process_output_files\"][0])\n",
    "print(f\"input query vector: {query_vector[0]}\")\n",
    "print(f\"closest vector from original: {original_vectors[indices[0][0]]}\")\n",
    "print(f\"distance from query to this vector: {distances[0][0]}\")\n",
    "print(f\"second closest vector from original: {original_vectors[indices[0][1]]}\")\n",
    "print(f\"distance from query to this vector: {distances[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `semantic_search` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "krixik's `semantic_search` method is a convenience function for both embedding and querying - and so can only be used with pipelines containing both `text-embedder` and `vector-db` modules in succession.\n",
    "\n",
    "Below we construct the simplest custom pipeline that satisfies this criteria - a standard vector search pipeline consisting of three modules: a `parser`, `text-embedder`, and `vector-db` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom pipeline object\n",
    "pipeline = krixik.create_pipeline(\n",
    "    name=\"vector-search-pipeline-check\",\n",
    "    module_chain=[\"parser\", \"text-embedder\", \"vector-db\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "remove_convert"
    ]
   },
   "outputs": [],
   "source": [
    "reset_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform any of the core system methods on our custom pipeline (e.g., [`process`](../system/process.md), [`list`](../system/list.md), etc.,).  Additionally we can invoke the [`semantic_search`](../system/semantic_search.md) method.\n",
    "\n",
    "Lets first process a file with our new pipeline.  The `vector-db` module takes in a text file, and returns `faiss` vector database consisting of all non-trivial `(snippet, line_numbers)` tuples from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"status_code\": 200,\\n  \"pipeline\": \"vector-search-pipeline-check\",\\n  \"request_id\": \"905f1a09-06c8-4c74-977b-2144a1f4f6a9\",\\n  \"file_id\": \"04281f52-26db-431f-87c7-7675dd355c99\",\\n  \"message\": \"SUCCESS - output fetched for file_id 04281f52-26db-431f-87c7-7675dd355c99.Output saved to location(s) listed in process_output_files.\",\\n  \"warnings\": [],\\n  \"process_output\": null,\\n  \"process_output_files\": [\\n    \"../../data/output/04281f52-26db-431f-87c7-7675dd355c99.faiss\"\\n  ]\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/1984_very_short.txt\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=test_file,\n",
    "    local_save_directory=\"../../data/output\",  # save output repo data output subdir\n",
    "    expire_time=60 * 10,  # set all process data to expire in 10 minutes\n",
    "    wait_for_process=True,  # wait for process to complete before regaining ide\n",
    "    verbose=False,\n",
    ")  # set verbosity to False\n",
    "\n",
    "# nicely print the output of this process\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query our text with natural language as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"dff05e88-1432-4760-9ab1-5ec9210b8f51\",\n",
      "  \"message\": \"Successfully queried 1 user file.\",\n",
      "  \"warnings\": [],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"0ac060c7-c39b-4287-93f2-184335e5cdea\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"krixik_generated_file_name_zgulrqdfmu.txt\",\n",
      "        \"symbolic_directory_path\": \"/etc\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_vectors\": 2,\n",
      "        \"created_at\": \"2024-05-03 22:50:11\",\n",
      "        \"last_updated\": \"2024-05-03 22:50:11\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.236\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "          \"line_numbers\": [\n",
      "            2,\n",
      "            3,\n",
      "            4,\n",
      "            5\n",
      "          ],\n",
      "          \"distance\": 0.429\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform semantic_search over the input file\n",
    "semantic_output = pipeline.semantic_search(query=\"it was cold night\", file_ids=[process_output[\"file_id\"]])\n",
    "\n",
    "# nicely print the output of this process\n",
    "print(json.dumps(semantic_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
