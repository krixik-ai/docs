{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Module Pipeline: Semantically-Searchable Translated Transcription\n",
    "\n",
    "This document details a modular pipeline that takes in an audio file, [`transcribes`](../../modules/ai_model_modules/transcribe_module.md) it, [`translates`](../../modules/ai_model_modules/translate_module.md) the transcription into a desired language, and makes the result [`semantically searchable`](../../system/search_methods/semantic_search_method.md).\n",
    "\n",
    "The document is divided into the following sections:\n",
    "\n",
    "- [Pipeline Setup](#pipeline-setup)\n",
    "- [Processing an Input File](#processing-an-input-file)\n",
    "- [Performing Semantic Search](#performing-semantic-search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "# import utilities\n",
    "import sys \n",
    "import json\n",
    "import importlib\n",
    "sys.path.append('../../../')\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n",
    "\n",
    "# load secrets from a .env file using python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"../../../.env\")\n",
    "MY_API_KEY = os.getenv('MY_API_KEY')\n",
    "MY_API_URL = os.getenv('MY_API_URL')\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "krixik.init(api_key = MY_API_KEY, \n",
    "            api_url = MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Setup\n",
    "\n",
    "To achieve what we've described above, let's set up a pipeline sequentially consisting of the following modules:\n",
    "\n",
    "- A [`transcribe`](../../modules/ai_model_modules/transcribe_module.md) module.\n",
    "\n",
    "- A [`translate`](../../modules/ai_model_modules/translate_module.md) module.\n",
    "\n",
    "- A [`json-to-txt`](../../modules/support_function_modules/json-to-txt_module.md) module.\n",
    "\n",
    "- A [`parser`](../../modules/support_function_modules/parser_module.md) module.\n",
    "\n",
    "- A [`text-embedder`](../../modules/ai_model_modules/text-embedder_module.md) module.\n",
    "\n",
    "- A [`vector-db`](../../modules/database_modules/vector-db_module.md) module.\n",
    "\n",
    "We use the [`json-to-txt`](../../modules/support_function_modules/json-to-txt_module.md) and [`parser`](../../modules/support_function_modules/parser_module.md) combination, which combines the transcribed snippets into one document and then splices it again, to make sure that any pauses in speech don't make for partial snippets that can confuse the [`text-embedder`](../../modules/ai_model_modules/text-embedder_module.md) model.\n",
    "\n",
    "Pipeline setup is accomplished through the [`.create_pipeline`](../../system/pipeline_creation/create_pipeline.md) method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline as detailed above\n",
    "pipeline = krixik.create_pipeline(name=\"multi_semantically_searchable_translated_transcription\",\n",
    "                                  module_chain=[\"transcribe\",\n",
    "                                                \"translate\",\n",
    "                                                \"json-to-txt\",\n",
    "                                                \"parser\",\n",
    "                                                \"text-embedder\",\n",
    "                                                \"vector-db\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing an Input File\n",
    "\n",
    "Lets take a quick look at a test file before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../../../data/input/deadlift.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine contents of input file\n",
    "import IPython\n",
    "IPython.display.Audio(\"../../../data/input/deadlift.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the input text is in Spanish, we'll use the (non-default) [`opus-mt-es-en`](https://huggingface.co/Helsinki-NLP/opus-mt-es-en) model of the [`translate`](../../modules/ai_model_modules/translate_module.md) module to translate it into English.\n",
    "\n",
    "We will use the default models for every other module in the pipeline, so they don't have to be specified in the [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument of the [`.process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the file through the pipeline, as described above\n",
    "process_output = pipeline.process(local_file_path = \"../../../data/input/deadlift.mp3\", # the initial local filepath where the input file is stored\n",
    "                                  local_save_directory=\"../../../data/output\", # the local directory that the output file will be saved to\n",
    "                                  expire_time=60*30, # process data will be deleted from the Krixik system in 30 minutes\n",
    "                                  wait_for_process=True, # wait for process to complete before returning IDE control to user\n",
    "                                  verbose=False, # do not display process update printouts upon running code\n",
    "                                  modules={\"translate\": {\"model\": \"opus-mt-es-en\"}}) # specify a non-default model for use in the translate module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below. To learn more about each component of the output, review documentation for the [`.process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method.\n",
    "\n",
    "Because the output of this particular module-model pair is a [FAISS](https://github.com/facebookresearch/faiss) database file, `process_output` is \"null\". However, the output file has been saved to the location noted in the `process_output_files` key.  The `file_id` of the processed input is used as a filename prefix for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"multi_semantically_searchable_translated_transcription\",\n",
      "  \"request_id\": \"6b26a79b-b322-4c69-a3d4-77358545e045\",\n",
      "  \"file_id\": \"611de878-2c15-4833-bfc3-69073125a3ad\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id 611de878-2c15-4833-bfc3-69073125a3ad.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../../data/output/611de878-2c15-4833-bfc3-69073125a3ad.faiss\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Semantic Search\n",
    "\n",
    "Krixik's [`.semantic_search`](../../system/search_methods/semantic_search_method.md) method enables semantic search on documents processed through certain pipelines. Given that the [`.semantic_search`](../../system/search_methods/semantic_search_method.md) method both [embeds](../../modules/ai_model_modules/text-embedder_module.md) the query and performs the search, it can only be used with pipelines containing both a [`text-embedder`](../../modules/ai_model_modules/text-embedder_module.md) module and a [`vector-db`](../../modules/database_modules/vector-db_module.md) module in immediate succession.\n",
    "\n",
    "Since our pipeline satisfies this condition, it has access to the [`.semantic_search`](../../system/search_methods/semantic_search_method.md) method. Let's use it to query our text with natural language, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"4927d817-7d3b-4c96-8301-dd75a77ac045\",\n",
      "  \"message\": \"Successfully queried 1 user file.\",\n",
      "  \"warnings\": [],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"611de878-2c15-4833-bfc3-69073125a3ad\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"krixik_generated_file_name_bnzvpcikcl.mp3\",\n",
      "        \"symbolic_directory_path\": \"/etc\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_vectors\": 8,\n",
      "        \"created_at\": \"2024-05-20 06:39:38\",\n",
      "        \"last_updated\": \"2024-05-20 06:39:38\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"snippet\": \"To make movement, we have to put the columns in front of the bar and start to move the Shoulders and the Shoulders together.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.303\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"During all movement, we want to see the bar close to the body, to move and down.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.355\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"When the bar goes to the knees, we have the correct angle of our trunk and we double the knees until the head is properly aligned to position the knee and rest.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.357\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"When the bar goes to the knees, we extend the knee.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.374\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"To begin, we want to see the feet in the anchors of the chair and the men, the columns in the ground, a neutral column, mediated by the abdomen, the men are going to go through there.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.377\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform semantic_search over the file in the pipeline\n",
    "semantic_output = pipeline.semantic_search(query=\"be sure to hold the weights very firmly\", \n",
    "                                           file_ids=[process_output[\"file_id\"]])\n",
    "\n",
    "# nicely print the output of this process\n",
    "print(json.dumps(semantic_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
