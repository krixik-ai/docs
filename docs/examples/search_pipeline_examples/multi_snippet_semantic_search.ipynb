{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Module Pipeline: Semantic Search on Snippets\n",
    "\n",
    "This document details a modular pipeline that takes in a series of text snippets in a JSON file and enables [`semantic search`](../../system/search_methods/semantic_search_method.md) on them.\n",
    "\n",
    "The document is divided into the following sections:\n",
    "\n",
    "- [Pipeline Setup](#pipeline-setup)\n",
    "- [Processing an Input File](#processing-an-input-file)\n",
    "- [Performing Semantic Search](#performing-semantic-search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "# import utilities\n",
    "import sys \n",
    "import json\n",
    "import importlib\n",
    "sys.path.append('../../../')\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n",
    "\n",
    "# load secrets from a .env file using python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"../../../.env\")\n",
    "MY_API_KEY = os.getenv('MY_API_KEY')\n",
    "MY_API_URL = os.getenv('MY_API_URL')\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "krixik.init(api_key = MY_API_KEY, \n",
    "            api_url = MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Setup\n",
    "\n",
    "To achieve what we've described above, let's set up a pipeline sequentially consisting of the following modules:\n",
    "\n",
    "- A [`text-embedder`](../../modules/ai_model_modules/text-embedder_module.md) module.\n",
    "\n",
    "- A [`vector-db`](../../modules/database_modules/vector-db_module.md) module.\n",
    "\n",
    "We do this by leveraging the [`.create_pipeline`](../../system/pipeline_creation/create_pipeline.md) method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline as detailed above\n",
    "\n",
    "pipeline_1 = krixik.create_pipeline(name=\"multi_snippets_semantic_search\",\n",
    "                                    module_chain=[\"text-embedder\",\n",
    "                                                  \"vector-db\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing an Input File\n",
    "\n",
    "Lets take a quick look at a test file before processing.\n",
    "\n",
    "The input format to this pipeline is a JSON file (given that it's the input format of its [first module](../../modules/ai_model_modules/text-embedder_module.md)). JSON input must always be in a [specific format](../../system/parameters_processing_files_through_pipelines/JSON_input_format.md), or the [`.process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\", \"line_numbers\": [1]}, {\"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\", \"line_numbers\": [2, 3, 4, 5]}]\n"
     ]
    }
   ],
   "source": [
    "# examine contents of input file\n",
    "\n",
    "with open(\"../../../data/input/1984_snippets.json\", \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the default models for every module in the pipeline, so the [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument of the [`.process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method doesn't need to be leveraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the file through the pipeline, as described above\n",
    "\n",
    "process_output_1 = pipeline_1.process(local_file_path = \"../../../data/input/1984_snippets.json\", # the initial local filepath where the input file is stored\n",
    "                                      local_save_directory=\"../../../data/output\", # the local directory that the output file will be saved to\n",
    "                                      expire_time=60*30, # process data will be deleted from the Krixik system in 30 minutes\n",
    "                                      wait_for_process=True, # wait for process to complete before returning IDE control to user\n",
    "                                      verbose=False) # do not display process update printouts upon running code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below. To learn more about each component of the output, review documentation for the [`.process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method.\n",
    "\n",
    "Because the output of this particular module-model pair is a [FAISS](https://github.com/facebookresearch/faiss) database file, the process output is null. However, the output file has been saved to the location noted in the `process_output_files` key.  The `file_id` of the processed input is used as a filename prefix for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"multi_snippets_semantic_search\",\n",
      "  \"request_id\": \"61cbeeee-2814-4b4e-abb8-fd91e1646398\",\n",
      "  \"file_id\": \"6b937587-a1e5-4a4d-9ccb-359b0300aac3\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id 6b937587-a1e5-4a4d-9ccb-359b0300aac3.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../../data/output/6b937587-a1e5-4a4d-9ccb-359b0300aac3.faiss\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "\n",
    "print(json.dumps(process_output_1, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Semantic Search\n",
    "\n",
    "Krixik's [`.semantic_search`](../../system/search_methods/semantic_search_method.md) method enables semantic search on documents processed through certain pipelines. Given that the [`.semantic_search`](../../system/search_methods/semantic_search_method.md) method both [embeds](../../modules/ai_model_modules/text-embedder_module.md) the query and performs the search, it can only be used with pipelines containing both a [`text-embedder`](../../modules/ai_model_modules/text-embedder_module.md) module and a [`vector-db`](../../modules/database_modules/vector-db_module.md) module in immediate succession.\n",
    "\n",
    "Since our pipeline satisfies this condition, it has access to the [`.semantic_search`](../../system/search_methods/semantic_search_method.md) method. Let's use it to query our text with natural language, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"c09c3021-995d-4628-8975-9c0919eac9fc\",\n",
      "  \"message\": \"Successfully queried 1 user file.\",\n",
      "  \"warnings\": [],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"6b937587-a1e5-4a4d-9ccb-359b0300aac3\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"krixik_generated_file_name_zxljpkkogi.json\",\n",
      "        \"symbolic_directory_path\": \"/etc\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_vectors\": 2,\n",
      "        \"created_at\": \"2024-05-20 06:42:24\",\n",
      "        \"last_updated\": \"2024-05-20 06:42:24\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.236\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "          \"line_numbers\": [\n",
      "            2,\n",
      "            3,\n",
      "            4,\n",
      "            5\n",
      "          ],\n",
      "          \"distance\": 0.429\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform semantic_search over the file in the pipeline\n",
    "\n",
    "semantic_output_1 = pipeline_1.semantic_search(query=\"it was cold night\",\n",
    "                                               file_ids=[process_output_1[\"file_id\"]])\n",
    "\n",
    "# nicely print the output of this process\n",
    "\n",
    "print(json.dumps(semantic_output_1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "\n",
    "reset_pipeline(pipeline_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
